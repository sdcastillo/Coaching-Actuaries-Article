---
title: "Actuarial Exams"
author: "Sam Castillo"
date: "February 3, 2018"
output: 
  html_document:
    toc: true
    toc_depth: 2
    theme: flatly
---

```{r global_options, warning= F, message= F, echo = F}
suppressMessages({
library(DT)
require(lubridate)
library(tidyr)
library(randomForest)
library(e1071)
library(ROCR)
library(pROC)
library(pdp)
library(AppliedPredictiveModeling)
library(mice)
library(Amelia)
library(caret)
library(DMwR)
library(ROSE)
library(corrplot)
library(randomForest)
library(prettyR)
library(readxl)
library(VIM)#missing data analysis
library(tidyverse)#always load last
  
#data table options
opts = list(searching = F)

#load data source
load("adapt_data.Rda")

#round numeric columns for prettier datatables
adapt_data = adapt_data %>% 
  mutate_if(is.numeric, funs(round(., 1))) %>% 
  mutate(q_ordinal = as.numeric(q_ordinal),
          #sunday is 1
         numeric_weekday = zoo::index(weekday,levels(adapt_data$weekday)) + 2)
})

#for some models, the levels need to be valid variable names.  This means that they can't be "TRUE"/"FALSE"
levels(adapt_data$correct) <- c("NO", "YES")
levels(adapt_data$marked) <- c("NO", "YES")

#describe what the features are
feature_schema = read_excel('data_schema.xlsx')

#make a graph showing pairwise dotplots
my_pairs_plot = function(cur_features){
  transparentTheme(trans = .9)
  caret::featurePlot(x = cur_features,
            y = cur_features$correct,
            plot = "pairs",
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "+",
            auto.key = list(columns = 2))
}

#make a graph showing density
my_density_plot = function(cur_features){
  transparentTheme(trans = .9)
  featurePlot(x = cur_features, 
            y = adapt_data$correct,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
             scales = list(x = list(relation="free"), 
                           y = list(relation="free")), 
             adjust = 1.5, 
            auto.key = list(columns = 2)
            )
}

#evaluate using the TEST data
eval_model = function(cur_name = "model",cur_model, cur_test_data){
  #somehow this now works??  The documentation on roxygen is out of date
  predictor = as.numeric(predict.train(object = cur_model, new  = cur_test_data %>% select(-correct), unkOnly = T))
  label = as.numeric(cur_test_data$correct)
  out <- confusionMatrix(label, predictor)
 data_frame("Model" = cur_name,
            'ROC' = roc(label, predictor)$auc,
       'Test Set Accuracy' = out$overall[1])
}
```
##1. Introduction

The actuarial profession has been rated highly for decades for low stress levels, high compensation, an interesting work environment, and job security; however, the elephant in the room is the rigorous actuarial exam requirements.  There are 10 exams in order to gain a Fellow credential, and each of these have pass rates between 40-50%.  There is a great deal of literature and study tutorials based on anectdotal evidence, offering prescriptive study methods and study schedules, but very little hard data.  This analysis uses statistical learning techniques to shed light onto which factors contribute to actuarial exam success.

This data was provided by [CoachingActuaries.com](coachingactuaries.com), from my personal online-learning accounts for exams P/1, FM/2, and MFE/3F with the Adapt product.  This article would not have been possible without the support of Ben Kester, [[[other names]]] who graciously agreed to send me the data and publish the result.  I am not being compensated in any way for writing this article.

The premiminary actuarial exams consist of 3-hour, multiple-choice, computer-based math tests offered once every six months.  There are 30-35 questions for the first three exams.  My experience has been with exam P, covering probability theory, exam FM, covering financial mathematics, and exam MFE (soon to be IMF), covering models for financial economics. 

Adapt is an automatic learning engine which generates problems to fit a user's current skill level.  These questions are intented to mimic the questions on real exams.  The adapt subscription includes access to a practice test bank, an exam simulator, tutorial videos, a user forum, and performance feedback.  Each user is given an Earned Level, which ranges from 1 - 10 and increases as they answer more difficult questions correctly.  Adapt claims that users with an earned level above 7 have over a 90% pass percentage based on post-exam surveys. *[check this claim]*  For exams P, FM, and MFE, my account logged 36.7, 60.1, and 54.8 hours of practice respectively. These numbers are only estimates, as there is no clear measure for active screen time.  Time spent on the Adapt product represents only a fraction of my total study time, which included off-screen reviews and textbook practice.  The Society of Actuaries (SOA) recommends 100 study hours per hour of exam, or 300 for each of these three exams.

The data and R code for generating this article can be downloaded [here](insert link).  

##2. Research Findings

 - The time spent per problem is the most significant factor impacting whether a question is correct.  Time is a valuable currency in a 30-question exam.  In every model which was tested, the time spent per problem was most important.  The optimal time per problem is about 5.5 minutes for the single-question level.
 
 - Question difficulty as Adapt rates it does have an immediate negative correlation with correct outcomes.  Simply because other people find a topic difficult does not imply that the person taking the exam should.  Difficulty can be overcome with practice.  When looking at the partial dependence plots, the reverse is sometimes true.
 
 - Practice questions are less useful when the user gets them wrong.  For each specific sub-category, these models calculated a running total of the problem difficulty, added to the total when a question was answered correctly and subtracting when incorrect.  This feature could be used to predict whether any given question would be answered correctly.
 
 - Successful practice uses questions which are of greater difficulty than needed.  Weight-lifters practice with heavier than compitition weights, baseball players practice-swing with weighted bats, martial artists puch bricks to strengthen their hands, and actuarial students should solve math problems more difficult than needed to prepare for the real exam.  These models used a feature which filters out questions of a lesser difficulty and found this to be significant in predicting question outcomes.

##3. Data Organization

###Original Quantitative Features

These data represent only a single individual's experience, and these results have not been verified against a broader population. Ther

[Make a graphic showing the modeling process]

[raw data --> cleaned --> transformed --> visualization --> models --> interpretation]

The data consists of question-level detail for the online course. There are around 1500 math problems from my personal account, with details such as whether it was correct or not, the time spent on it, the curriculumn category, and so forth.

```{r echo = F}
feature_schema %>% 
  filter(!str_detect(`Feature Name`, "hist")) %>% 
  datatable(options = opts)
```



Here is a graph of my study time.  Quizzes are non timed, and are customized.  Exams are limited to 3 hours, 30 questions from a simulated topic, and I tried to not cheat during these (e.g., look at notes, pause the exam, phone a friend, etc). 

Earned Level

```{r echo = F, fig.height= 4}
#[pretty timeline graph for P, FM, MFE.  Use P == blue, FM == green, MFE == orange]
p = adapt_data %>% 
  filter(course == "P")

fm = adapt_data %>% 
  filter(course == "FM")

mfe = adapt_data %>% 
  filter(course == "MFE")

p.plot <- p %>% 
      filter(exam_type == "e") %>% 
      group_by(creation_dt) %>% 
      summarise(EL = max(EL_begin)) %>% 
      ggplot(aes(creation_dt, EL)) +
      geom_line(color = "darkblue") + 
      ylim(0,10)

fm.plot <- fm %>% 
      filter(exam_type == "e") %>% 
      group_by(creation_dt) %>% 
      summarise(EL = max(EL_begin)) %>% 
      ggplot(aes(creation_dt, EL)) +
      geom_line(color = "darkgreen") + 
      ylim(0,10)

mfe.plot <- mfe %>% 
      filter(exam_type == "e") %>% 
      group_by(creation_dt) %>% 
      summarise(EL = max(EL_begin)) %>% 
      ggplot(aes(creation_dt, EL)) +
      geom_line(color = "orange") + 
      ylim(0,10)

grid.arrange(p.plot, fm.plot, mfe.plot, nrow = 1)
```

The data supports this advice.  Not too surprisingly, the two most importance factors to whether or not any given question is likely to be correct are the difficulty, and the amount of time spent on it.  

The simplest way to asnwer this is to look at the correlations between features.  Is there any input which is consistently different when the question is correct verses incorrect?  As you can see below, there is a strong negative correlation with `difficulty`.    Interestingly, `minutes_used`, and `remaining_exam_time` are both negative as well.  This is less clear as to why; at this point we do not want to confuse correlation with causation.

The variable `EL_change` is the change in earned level, which is an Adapt-specific rank from 1-10 [define this earlier].  Note the negative correlation with `weekday`.  This could be due to my history of taking practice exams during certain days of the week.

```{r echo = F, fig.width= 4, fig.height= 4}
numeric_index <- sapply(adapt_data, is.numeric)
numeric_cols = adapt_data[,numeric_index] %>%
  mutate(correct = as.numeric(adapt_data$correct)) %>%
  select(correct, everything(),- contains("hist"), -EL_begin, -EL_end, -nth_exam, - nth_e_or_q, -hrs_since_previous_e) %>%
  rename(remaining_exam_time = approx_remaining_time,
         weekday = numeric_weekday) %>% 
  as.matrix() 
corrplot(cor(numeric_cols),method = "square")
```

Class separation is how well the x-features separate out the class label, `correct` in this case.  

```{r echo = F}
numeric_cols %>%
  as_data_frame() %>% 
  as.matrix() %>% 
  my_density_plot()
```
###Original Categorical Features

[show a few question categories, subcategories.  Talk about question tags]

###Engineered Historical Features

I generated additional features and used random forest importance rankings, class-separation plots, and improved accuracy to decide which features to keep.

```{r echo = F}
feature_schema %>% 
  filter(str_detect(`Feature Name`, "hist")) %>% 
  datatable(options = opts)
```

As the below plot shows, there is improved class separation for several of the historical features.  Intuitively, this implies that my likelihood of answering a question correctly depends on the number of questions which I have answered in the past in the same category.  Not all of these features are useful, and in fact this graph is only showing a subset of the 13 historical features tested.

```{r echo = F}
adapt_data %>%
  select(contains("hist"), - hist_tot_time_correct, - hist_subcat_diff, - hist_total_time_q, - hist_total_time_q, - hist_cat_n, - hist_n_marked) %>%
  my_density_plot()
```

##4. Modeling

The Receiver Operating Characteristic (ROC) metric was used for model selection. A 75%-25% validation set split was first created, and then 10-fold cross validation was used for model training.  The numeric variables were pre-processed with to be scaled and centered.  

Missing values were determined as those questions where `minutes_used` was equal to zero, or less than 10 seconds.

[fill in paragraph speaking to cat1 missingness and state the number of cases you dropped in total, why they are not important, etc.  MCAR?]

```{r}
set.seed(400)

#drop columns which have no predictive power
#we only care about predicting exam outcomes
model_data <- adapt_data %>% 
  filter(exam_type == "e") %>% 
  select(- questionID, - examID, - exam_type, - creation_dt_time, - creation_dt) %>% 
  ungroup()

#these features are ok
best_subset <-  model_data %>% 
  ungroup() %>% 
  mutate_at(c("cat1", "cat2", "cat3", "subcat1", "subcat3", "subcat3"), funs(droplevels)) %>% 
  #factor levels mess up models
  dplyr::select( correct, difficulty, marked, minutes_used, hist_greater_diff, hist_net_diff, approx_remaining_time, creation_hr, cat1, hist_repeat_question)

#this is the easily-interpretible data
#this splits to preserve the ORIGINAL class distribution, but doesn't over/undersample
index_train <- createDataPartition(y = model_data$correct, p = 0.75, list = FALSE)

#these have worked in the past
best_training <- best_subset[index_train,]
best_testing <- best_subset[-index_train,]

training <- model_data[index_train,]
testing <- model_data[-index_train,]

#this is to improve accuracy for knn and logit
preproc.values <- preProcess(x = best_subset, method = c("center", "scale"))
best_transformed = predict(preproc.values, newdata = best_subset) 

training_transformed <- best_transformed[index_train,]
testing_transformed <- best_transformed[-index_train,]
```

A Naive Bayes baseline model gives an accuracy of 73% and AUC of 0.6388.  This was only using the features x, y, and z.  When creating the historical features, a random forest was used for variable importance.

The below partial dependency plots of the random forest model show how the probability of answering a question correctly changes with the input levels.  

```{r}
train_control <- trainControl(method = "cv", number=10)
#nb1 <- train(correct ~., data = training_transformed %>% select(-cat1), trControl=train_control, method="nb")

#nb_metrics = eval_model("Naive Bayes", nb1, testing_transformed)
```



Logistic Regression

```{r}
train_control<- trainControl(method="cv", number=10)

f1 = correct ~ difficulty + minutes_used
logit1<- train(f1, data = training_transformed, trControl=train_control, method="glm", family=binomial())

f2 = correct ~ difficulty + minutes_used  + approx_remaining_time + creation_hr
logit2 <- train(f2, data = training_transformed, trControl = train_control, method="glm", family=binomial())

f3 = correct ~ difficulty + marked + minutes_used +  hist_greater_diff + approx_remaining_time + creation_hr + cat1
logit3 <- train(f3, data = training_transformed, trControl = train_control, method="glm", family=binomial())

f4 = correct ~ difficulty + minutes_used  + hist_repeat_question
logit4 <- train(f4, data = training_transformed, trControl = train_control, method="glm", family=binomial())


logit_metrics = eval_model("Logistic Regression", logit3, testing_transformed)
```

K-nearest-neighbor

```{r}
ctrl <- trainControl(method="repeatedcv", repeats = 3)
#knn1 <- train(correct ~ ., data = training_transformed, method = "knn", trControl = ctrl, tuneLength = 20)
#knn_metrics = eval_model("K_Nearest Neighbor", knn1, testing_transformed)
```

Random Forest

```{r}
# Create model with default paramters
control <- trainControl(method = "repeatedcv", classProbs = T, number=10, repeats=3)
seed <- 7; set.seed(seed)
metric <- "Accuracy"
mtry <- sqrt(ncol(training)) # mtry can be increased at the risk of over-fitting
tunegrid <- expand.grid(.mtry=mtry)

#using all variables
#rf1 <- train(correct ~., data = training , method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

mtry <- sqrt(ncol(best_training)) # mtry can be increased at the risk of over-fitting
tunegrid <- expand.grid(.mtry=mtry)

#using a subset of variables
#rf2 <- train(correct ~., data = best_training , method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#rf_metrics = eval_model("Random Forest", rf2, best_testing)
#plot(varImp(rf2), top = 20)
```

[Model selection comparison]

```{r eval = F}
metrics_data = rbind(nb_metrics, logit_metrics, knn_metrics, rf_metrics)
metrics_data %>% 
  arrange(desc(ROC))
```


###Model Selection

[model output/interpretation section]

###Model Interpretation

```{r eval = F}

difficulty.plot <- rf1 %>% 
  pdp::partial(pred.var = "difficulty") %>% 
  autoplot(rug = T, train = best_subset) + 
  scale_y_reverse()

minutes_used.plot <- rf1 %>% 
  pdp::partial(pred.var = "minutes_used") %>% 
  autoplot(rug = T, train = best_subset) + 
  scale_y_reverse() 

hist1 <- rf1 %>% 
  pdp::partial(pred.var = "hist_greater_diff") %>% 
  autoplot(rug = T, train = best_subset) + 
  scale_y_reverse()

hist2 <- rf1 %>% 
  pdp::partial(pred.var = "hist_net_diff") %>% 
  autoplot(rug = T, train = best_subset) + 
  scale_y_reverse()

hist3 <- rf1 %>% 
  pdp::partial(pred.var = "hist_net_diff") %>% 
  autoplot(rug = T, train = best_subset) + 
  scale_y_reverse()

grid.arrange(difficulty.plot, minutes_used.plot, hist1, hist2, hist3)
```
##4.  Conclusion
